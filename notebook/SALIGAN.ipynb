{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## データセットの実装",
   "id": "253fafb491308345"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class SALICONDataset(data.Dataset):\n",
    "    def __init__(self, root_dataset_dir, val_mode=False):\n",
    "        \"\"\"\n",
    "        SALICONデータセットを読み込むためのDatasetクラス\n",
    "        \n",
    "        Parameters:\n",
    "        -----------------\n",
    "        root_dataset_dir : str\n",
    "            SALICONデータセットの上のディレクトリのパス\n",
    "        val_mode : bool (default: False)\n",
    "            FalseならばTrainデータを、TrueならばValidationデータを読み込む\n",
    "        \"\"\"\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "        self.imgsets_dir = os.path.join(self.root_dataset_dir, 'SALICON/image_sets')\n",
    "        self.img_dir = os.path.join(self.root_dataset_dir, 'SALICON/imgs')\n",
    "        self.distribution_target_dir = os.path.join(self.root_dataset_dir, 'SALICON/algmaps')\n",
    "        self.img_tail = '.jpg'\n",
    "        self.distribution_target_tail = '.png'\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.Resize((192, 256)), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "        self.distribution_transform = transforms.Compose([transforms.Resize((192, 256)), transforms.ToTensor()])\n",
    "\n",
    "        if val_mode:\n",
    "            train_or_val = \"val\"\n",
    "        else:\n",
    "            train_or_val = \"train\"\n",
    "        imgsets_file = os.path.join(self.imgsets_dir, '{}.txt'.format(train_or_val))\n",
    "        files = []\n",
    "        for data_id in open(imgsets_file).readlines():\n",
    "            data_id = data_id.strip()\n",
    "            img_file = os.path.join(self.img_dir, '{0}{1}'.format(data_id, self.img_tail))\n",
    "            distribution_target_file = os.path.join(self.distribution_target_dir,\n",
    "                                                    '{0}{1}'.format(data_id, self.distribution_target_tail))\n",
    "            files.append({\n",
    "                'img': img_file,\n",
    "                'distribution_target': distribution_target_file,\n",
    "                'data_id': data_id\n",
    "            })\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -----------\n",
    "        data : list\n",
    "            [img, distribution_target, data_id]\n",
    "        \"\"\"\n",
    "        data_file = self.files[index]\n",
    "        data = []\n",
    "\n",
    "        img_file = data_file['img']\n",
    "        img = Image.open(img_file)\n",
    "        data.append(img)\n",
    "\n",
    "        distribution_target_file = data_file['distribution_target']\n",
    "        distribution_target = Image.open(distribution_target_file)\n",
    "        data.append(distribution_target)\n",
    "\n",
    "        # transform\n",
    "        data[0] = self.transform(data[0])\n",
    "        data[1] = self.distribution_transform(data[1])\n",
    "\n",
    "        data.append(data_file['data_id'])\n",
    "        return data\n"
   ],
   "id": "5015e45d2008ca2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Cat2000Dataset(Dataset):\n",
    "    def __init__(self, categories: Optional[list[str]] = None, transform_module=None, download_path: str = \"\",\n",
    "                 download: bool = True):\n",
    "        if categories is None:\n",
    "            categories = [\"*\"]\n",
    "        self.download_path = download_path\n",
    "        self.dataset_path = os.path.join(self.download_path, \"trainSet\", \"Stimuli\")\n",
    "        self.categories = categories\n",
    "        self.transform = transform_module\n",
    "\n",
    "        if download and not self.is_exist_dataset():\n",
    "            self.download_dataset()\n",
    "\n",
    "        # 画像とマップのペアを取得\n",
    "        self.image_map_pairs = self.get_image_map_paths()\n",
    "\n",
    "    def is_exist_dataset(self):\n",
    "        return os.path.exists(self.dataset_path)\n",
    "\n",
    "    def download_dataset(self):\n",
    "        \"\"\"データセットをダウンロードし、必要に応じて解凍\"\"\"\n",
    "        zip_path = os.path.join(self.download_path, \"trainSet.zip\")\n",
    "        url = \"http://saliency.mit.edu/trainSet.zip\"\n",
    "\n",
    "        if os.path.exists(self.dataset_path):\n",
    "            print(f\"Dataset already exists at {self.dataset_path}, skipping download.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Downloading dataset from {url}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()  # HTTPエラーを確認\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            downloaded_size = 0\n",
    "\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True, dynamic_ncols=True) as progress:\n",
    "                with open(zip_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=128):\n",
    "                        downloaded_size += len(chunk)\n",
    "                        f.write(chunk)\n",
    "                        progress.update(len(chunk))\n",
    "\n",
    "            print(\"\\nDownload completed.\")\n",
    "\n",
    "            # ZIPファイルを解凍\n",
    "            print(f\"Unzipping {zip_path}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.download_path)\n",
    "            print(f\"Extracted to {self.download_path}.\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during download: {e}\")\n",
    "\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"Error: Bad zip file.\")\n",
    "\n",
    "    def get_image_map_paths(self):\n",
    "        result = []\n",
    "        for category in self.categories:\n",
    "            # 画像ファイルのパスを取得\n",
    "            image_paths = glob.glob(os.path.join(self.dataset_path, category, \"*.jpg\"))\n",
    "            for image_path in image_paths:\n",
    "                # ベース名を取得してマップファイルのパスを生成\n",
    "                base_name = os.path.basename(image_path)\n",
    "                map_name = base_name.replace(\".jpg\", \"_SaliencyMap.jpg\")\n",
    "                map_path = os.path.join(self.dataset_path, category, \"Output\", map_name)\n",
    "\n",
    "                if os.path.exists(map_path):\n",
    "                    result.append((image_path, map_path))\n",
    "                else:\n",
    "                    print(f\"Warning: No corresponding map found for {image_path}\")\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_map_pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_path, map_path = self.image_map_pairs[idx]\n",
    "        image, map_image = self.convert_to_tensor(image_path, map_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            map_image = self.transform(map_image)\n",
    "\n",
    "        return image, map_image\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_tensor(image_path, map_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        map_image = Image.open(map_path).convert(\"RGB\")\n",
    "\n",
    "        return image, map_image\n",
    "\n",
    "    def __str__(self):\n",
    "        buffer = []\n",
    "        for index in range(len(self)):\n",
    "            image, map_image = self[index]\n",
    "            buffer.append(f\"image: {image.size}, map: {map_image.size}\")\n",
    "\n",
    "        return \"\\\\n\".join(buffer)\n"
   ],
   "id": "d8496ad636ac0b64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## モデルの実装",
   "id": "8cbf757d3bab0716"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 学習とテストの実装",
   "id": "3e18e5ef81222595"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.encoder_first = torchvision.models.vgg16(pretrained=pretrained).features[:17]  # 重み固定して使う部分\n",
    "        self.encoder_last = torchvision.models.vgg16(pretrained=pretrained).features[17:-1]  # 学習する部分\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 1, 1, padding=0),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder_first(x)\n",
    "        x = self.encoder_last(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(4, 3, 1, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 32 * 24, 100, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 2, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2, 1, bias=True),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ],
   "id": "b0b855e8604619d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 学習の実装",
   "id": "f3665d02e5e943f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#-----------------\n",
    "# SETTING\n",
    "root_dataset_dir = \"\"  # SALICONデータセットの上のディレクトリのパス\n",
    "alpha = 0.005  # Generatorの損失関数のハイパーパラメータ。論文の推奨値は0.005\n",
    "epochs = 120\n",
    "batch_size = 32  # 論文では32\n",
    "#-----------------\n",
    "\n",
    "# 開始時間をファイル名に利用\n",
    "start_time_stamp = '{0:%Y%m%d-%H%M%S}'.format(datetime.now())\n",
    "\n",
    "save_dir = \"./log/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# データローダーの読み込み\n",
    "train_dataset = SALICONDataset(\n",
    "    root_dataset_dir,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "                                           pin_memory=True, sampler=None)\n",
    "val_dataset = SALICONDataset(\n",
    "    root_dataset_dir,\n",
    "    val_mode=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True,\n",
    "                                         sampler=None)\n",
    "\n",
    "# モデルと損失関数の読み込み\n",
    "loss_func = torch.nn.BCELoss().to(DEVICE)\n",
    "generator = Generator().to(DEVICE)\n",
    "discriminator = Discriminator().to(DEVICE)\n",
    "\n",
    "# 最適化手法の定義（論文中の設定を使用）\n",
    "optimizer_G = torch.optim.Adagrad([\n",
    "    {'params': generator.encoder_last.parameters()},\n",
    "    {'params': generator.decoder.parameters()}\n",
    "], lr=0.0001, weight_decay=3 * 0.0001)\n",
    "optimizer_D = torch.optim.Adagrad(discriminator.parameters(), lr=0.0001, weight_decay=3 * 0.0001)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "    n_updates = 0  # イテレーションのカウント\n",
    "    n_discriminator_updates = 0\n",
    "    n_generator_updates = 0\n",
    "    d_loss_sum = 0\n",
    "    g_loss_sum = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        imgs = data[0]  # ([batch_size, rgb, h, w])\n",
    "        salmaps = data[1]  # ([batch_size, 1, h, w])\n",
    "\n",
    "        # Discriminator用のラベルを作成\n",
    "        valid = Variable(torch.FloatTensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False).to(DEVICE)\n",
    "        fake = Variable(torch.FloatTensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False).to(DEVICE)\n",
    "\n",
    "        imgs = Variable(imgs).to(DEVICE)\n",
    "        real_salmaps = Variable(salmaps).to(DEVICE)\n",
    "\n",
    "        # イテレーションごとにGeneratorとDiscriminatorを交互に学習\n",
    "        if n_updates % 2 == 0:\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            gen_salmaps = generator(imgs)\n",
    "\n",
    "            # Discriminatorへの入力用に元の画像と生成したSaliency Mapを結合して4チャンネルの配列を作る\n",
    "            fake_d_input = torch.cat((imgs, gen_salmaps.detach()), 1)  # ([batch_size, rgbs, h, w])\n",
    "\n",
    "            # Generatorの損失関数を計算\n",
    "            g_loss1 = loss_func(gen_salmaps, real_salmaps)\n",
    "            g_loss2 = loss_func(discriminator(fake_d_input), valid)\n",
    "            g_loss = alpha * g_loss1 + g_loss2\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            g_loss_sum += g_loss.item()\n",
    "            n_generator_updates += 1\n",
    "\n",
    "        else:\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Discriminatorへの入力用に元の画像と正解データのSaliency Mapを結合して4チャンネルの配列を作る            \n",
    "            real_d_input = torch.cat((imgs, real_salmaps), 1)  # ([batch_size, rgbs, h, w])\n",
    "\n",
    "            # Discriminatorの損失関数を計算\n",
    "            real_loss = loss_func(discriminator(real_d_input), valid)\n",
    "            fake_loss = loss_func(discriminator(fake_d_input), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            d_loss_sum += d_loss.item()\n",
    "            n_discriminator_updates += 1\n",
    "\n",
    "        n_updates += 1\n",
    "        if n_updates % 10 == 0:\n",
    "            if n_discriminator_updates > 0:\n",
    "                print(\n",
    "                    \"[%d/%d (%d/%d)] [loss D: %f, G: %f]\"\n",
    "                    % (epoch, epochs - 1, i, len(train_loader), d_loss_sum / n_discriminator_updates,\n",
    "                       g_loss_sum / n_generator_updates)\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"[%d/%d (%d/%d)] [loss G: %f]\"\n",
    "                    % (epoch, epochs - 1, i, len(train_loader), g_loss_sum / n_generator_updates)\n",
    "                )\n",
    "\n",
    "                # 重みの保存\n",
    "    # 5エポックごとと、最後のエポックを保存する\n",
    "    if ((epoch + 1) % 5 == 0) or (epoch == epochs - 1):\n",
    "        generator_save_path = '{}.pkl'.format(\n",
    "            os.path.join(save_dir, \"{}_generator_epoch{}\".format(start_time_stamp, epoch)))\n",
    "        discriminator_save_path = '{}.pkl'.format(\n",
    "            os.path.join(save_dir, \"{}_discriminator_epoch{}\".format(start_time_stamp, epoch)))\n",
    "        torch.save(generator.state_dict(), generator_save_path)\n",
    "        torch.save(discriminator.state_dict(), discriminator_save_path)\n",
    "\n",
    "    # エポックごとにValidationデータの一部を可視化\n",
    "    with torch.no_grad():\n",
    "        print(\"validation\")\n",
    "        for i, data in enumerate(val_loader):\n",
    "            image = Variable(data[0]).to(DEVICE)\n",
    "            gen_salmap = generator(imgs)\n",
    "            gen_salmap_np = np.array(gen_salmaps.data.cpu())[0, 0]\n",
    "\n",
    "            plt.imshow(np.array(image[0].cpu()).transpose(1, 2, 0))\n",
    "            plt.show()\n",
    "            plt.imshow(gen_salmap_np)\n",
    "            plt.show()\n",
    "            if i == 1:\n",
    "                break\n"
   ],
   "id": "35c2acd6bb441083"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test Loop",
   "id": "ff0d7eb73e947a14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Google drive からfastText日本語モデル(vector_neologd.zip)をダウンロードする\n",
    "import requests\n",
    "\n",
    "URL = \"https://drive.google.com/uc?id=0ByFQ96A4DgSPUm9wVWRLdm5qbmc&export=download\"\n",
    "\n",
    "\n",
    "def request(url, file_id):\n",
    "    # ダウンロード画面のURL\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(url, params={'id': file_id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {'id': file_id, 'confirm': token}\n",
    "        return session.get(URL, params=params, stream=True)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_response_content(response, destination, chunk_size=32768):\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_id = 'TAKE ID FROM SHAREABLE LINK'\n",
    "    destination = './data/vector_neologd.zip'  # 保存先パスの指定\n",
    "    responce = request(file_id, destination)\n",
    "    save_response_content(responce, destination)\n"
   ],
   "id": "72ea377256be7a2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
