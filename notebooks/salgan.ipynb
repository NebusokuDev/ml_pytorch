{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## データローダーの実装",
   "id": "1378ba56a653cdea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import zipfile\n",
    "from logging import Logger, getLogger\n",
    "from os import mkdir\n",
    "from os.path import exists\n",
    "from typing import Callable\n",
    "from urllib.parse import urlparse\n",
    "from zipfile import BadZipFile\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from mpl_toolkits.mplot3d.proj3d import transform\n",
    "from requests import Response\n",
    "from torch import no_grad\n",
    "from torchvision.transforms.v2 import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "KIB = 2 ** 10\n",
    "\n",
    "\n",
    "class Downloader:\n",
    "    \"\"\"\n",
    "    指定されたURLからデータセットをダウンロードし、ZIPファイルを解凍するクラス。\n",
    "\n",
    "    :param root: ダウンロード先のルートディレクトリ。\n",
    "    :param url: ダウンロードするURLまたはURLを返す関数。\n",
    "    :param overwrite: 既存のファイルを上書きするかどうか（デフォルトはFalse）。\n",
    "    :param zip_filename: ZIPファイルの名前（指定がない場合はURLから取得）。\n",
    "    :param logger: ロギング用のLoggerオブジェクト（指定がない場合はデフォルトのLoggerを使用）。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: str, url: str | Callable[[], str] = None, overwrite: bool = False,\n",
    "                 zip_filename: str = None,\n",
    "                 logger: Logger = None):\n",
    "        \"\"\"\n",
    "        Downloaderのコンストラクタ。\n",
    "\n",
    "        :param root: ダウンロード先のルートディレクトリ。\n",
    "        :param url: ダウンロードするURLまたはURLを返す関数。\n",
    "        :param overwrite: 既存のファイルを上書きするかどうか（デフォルトはFalse）。\n",
    "        :param zip_filename: ZIPファイルの名前（指定がない場合はURLから取得）。\n",
    "        :param logger: ロギング用のLoggerオブジェクト（指定がない場合はデフォルトのLoggerを使用）。\n",
    "        \"\"\"\n",
    "        self._root = os.path.normpath(root)\n",
    "\n",
    "        if isinstance(url, Callable):\n",
    "            self.url = url()\n",
    "        else:\n",
    "            self.url = url\n",
    "\n",
    "        self.zip_filename = zip_filename or os.path.basename(urlparse(self.url).path)  # URLからファイル名を取得\n",
    "        self.zip_path = os.path.join(self._root, self.zip_filename)\n",
    "        self.extract_path = os.path.splitext(self.zip_path)[0]\n",
    "        self.overwrite = overwrite\n",
    "        self.logger = logger or getLogger(__name__)\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"データセットをダウンロードする。\n",
    "\n",
    "        既にデータセットがダウンロードされている場合、overwriteがFalseの場合はダウンロードをスキップします。\n",
    "        \"\"\"\n",
    "        if self.is_downloaded() and not self.overwrite:\n",
    "            print(f\"Dataset already exists at {self.zip_path}, skipping download.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Downloading dataset from {self.url}...\")\n",
    "\n",
    "        try:\n",
    "            response = self.request(self.url)\n",
    "            self.save_response_content(response, self.zip_path)\n",
    "\n",
    "            print(\"\\nDownload completed.\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during download: {e}\")\n",
    "\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"Error: Bad zip file.\")\n",
    "\n",
    "    def request(self, url):\n",
    "        \"\"\"指定されたURLにGETリクエストを送り、レスポンスを返す。\n",
    "\n",
    "        :param url: リクエストするURL。\n",
    "        :return: リクエストの結果得られたレスポンスオブジェクト。\n",
    "        \"\"\"\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # HTTPエラーを確認\n",
    "        return response\n",
    "\n",
    "    def save_response_content(self, response: Response, destination, chunk_size: int = 100 * KIB):\n",
    "        \"\"\"レスポンスのコンテンツを指定されたファイルに保存する。\n",
    "\n",
    "        :param response: 保存するためのレスポンスオブジェクト。\n",
    "        :param destination: 保存先ファイルのパス。\n",
    "        :param chunk_size: 保存時のチャンクサイズ（デフォルトは100KiB）。\n",
    "        \"\"\"\n",
    "        if not exists(self.root):\n",
    "            mkdir(self.root)\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in tqdm(response.iter_content(chunk_size=chunk_size)):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"ZIPファイルを解凍し、重複したルートフォルダがある場合はまとめる。\n",
    "\n",
    "        解凍先のディレクトリが既に存在する場合、その内容は保持されます。\n",
    "        \"\"\"\n",
    "        print(f\"Unzipping {self.zip_path}...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "                # ZIPファイルのトップレベルのフォルダを確認\n",
    "                top_level_dirs = {os.path.normpath(x).split(os.sep)[0] for x in zip_ref.namelist()}\n",
    "\n",
    "                if len(top_level_dirs) == 1:\n",
    "                    # トップレベルに1つのディレクトリだけある場合\n",
    "                    top_level_dir = next(iter(top_level_dirs))\n",
    "                    self.extract_path = os.path.join(self._root, top_level_dir)\n",
    "                    print(f\"Extracting into {self.extract_path}...\")\n",
    "\n",
    "                total_files = len(zip_ref.namelist())\n",
    "                with tqdm(total=total_files, unit='file') as progress:\n",
    "                    for file in zip_ref.namelist():\n",
    "                        destination = self._root if len(top_level_dirs) == 1 else self.extract_path\n",
    "                        zip_ref.extract(file, destination)\n",
    "                        progress.update(1)\n",
    "        except BadZipFile:\n",
    "            print(\"Error: Bad zip file, extraction failed.\")\n",
    "\n",
    "        print(f\"Extracted to {self._root}.\")\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        \"\"\"ダウンロード先のルートディレクトリを取得する。\"\"\"\n",
    "        return self._root\n",
    "\n",
    "    def is_downloaded(self):\n",
    "        \"\"\"ZIPファイルがダウンロードされているかどうかを確認する。\n",
    "\n",
    "        :return: ZIPファイルが存在する場合はTrue、それ以外はFalse。\n",
    "        \"\"\"\n",
    "        return os.path.exists(self.zip_path)\n",
    "\n",
    "    def is_extracted(self):\n",
    "        \"\"\"データセットが解凍されているかどうかを確認する。\n",
    "\n",
    "        :return: 解凍先が存在する場合はTrue、それ以外はFalse。\n",
    "        \"\"\"\n",
    "        return os.path.exists(self.extract_path)\n",
    "\n",
    "    def __call__(self, on_complete: Callable = None):\n",
    "        \"\"\"ダウンロードおよび解凍を実行する。\n",
    "\n",
    "        :param on_complete: 処理完了後に呼び出す関数（オプション）。\n",
    "        \"\"\"\n",
    "        if self.overwrite or not self.is_downloaded():\n",
    "            self.download()\n",
    "        else:\n",
    "            print(\"Dataset exists and 'overwrite' is False. No download.\")\n",
    "\n",
    "        if self.overwrite or not self.is_extracted():\n",
    "            self.extract()\n",
    "        else:\n",
    "            print(\"Dataset exists and 'overwrite' is False. No extract.\")\n",
    "\n",
    "        if on_complete is not None:\n",
    "            on_complete()"
   ],
   "id": "4a618cb00c105aef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets.downloader import Downloader\n",
    "\n",
    "\n",
    "class Cat2000(Dataset):\n",
    "    def __init__(self, categories: Optional[list[str]] = None,\n",
    "                 image_transform: Optional = None,\n",
    "                 map_transform: Optional = None,\n",
    "                 downloader: Optional[Downloader] = None,\n",
    "                 ):\n",
    "        if categories is None:\n",
    "            categories = [\"*\"]\n",
    "        self.categories = categories\n",
    "        self.image_transform = image_transform\n",
    "        self.map_transform = map_transform\n",
    "        self.downloader = downloader or Downloader(\"./data\", \"http://saliency.mit.edu/trainSet.zip\")\n",
    "        self.dataset_path = os.path.join(self.downloader.root, \"trainSet\", \"Stimuli\")\n",
    "\n",
    "        # 画像とマップのペアを取得\n",
    "        self.image_map_pair_cache = []\n",
    "        self.downloader(on_complete=self.cache_image_map_paths)\n",
    "\n",
    "    def cache_image_map_paths(self):\n",
    "        self.image_map_pair_cache = []\n",
    "\n",
    "        # categoriesにワイルドカードが含まれている場合、全カテゴリディレクトリを展開\n",
    "        if \"*\" in self.categories:\n",
    "            expanded_categories = [d for d in glob.glob(os.path.join(self.dataset_path, \"*\")) if os.path.isdir(d)]\n",
    "        else:\n",
    "            expanded_categories = [os.path.join(self.dataset_path, category) for category in self.categories]\n",
    "\n",
    "        # 展開したカテゴリディレクトリごとに処理を行う\n",
    "        for category_path in expanded_categories:\n",
    "            # 画像ファイルのパスを取得\n",
    "            image_paths = glob.glob(os.path.join(category_path, \"*.jpg\"))\n",
    "\n",
    "            for image_path in image_paths:\n",
    "                base_name = os.path.basename(image_path)\n",
    "                map_name = base_name.replace(\".jpg\", \"_SaliencyMap.jpg\")\n",
    "                map_path = os.path.join(category_path, \"Output\", map_name)\n",
    "\n",
    "                if os.path.exists(map_path):\n",
    "                    self.image_map_pair_cache.append((image_path, map_path))\n",
    "                else:\n",
    "                    print(f\"Warning: No corresponding map found for {map_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_map_pair_cache)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_path, map_path = self.image_map_pair_cache[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        map_image = Image.open(map_path).convert(\"RGB\")\n",
    "\n",
    "        if self.image_transform is not None:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "            if self.map_transform is not None:\n",
    "                map_image = self.map_transform(map_image)\n",
    "            else:\n",
    "                map_image = self.image_transform(map_image)\n",
    "\n",
    "        return image, map_image\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(\n",
    "            f\"image: {Image.open(pair[0]).size}, map: {Image.open(pair[1]).size}\" for pair in self.image_map_pair_cache)\n"
   ],
   "id": "6735eb0e39ba5932",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob\n",
    "from os import path\n",
    "from typing import Optional, Callable\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from datasets.downloader import Downloader\n",
    "\n",
    "\n",
    "class SALICONDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 val_mode: bool = False,\n",
    "                 image_transform: Optional[Callable] = None,\n",
    "                 map_transform: Optional[Callable] = None,\n",
    "                 images_downloader: Optional[Downloader] = None,\n",
    "                 map_downloader: Optional[Downloader] = None,\n",
    "                 ):\n",
    "\n",
    "        self.categories = \"val\" if val_mode else \"train\"\n",
    "\n",
    "        self.image_transform = image_transform\n",
    "        self.map_transform = map_transform\n",
    "\n",
    "        self.images_downloader = images_downloader or Downloader(\"./data/salicon\", \"\", zip_filename=\"images.zip\",\n",
    "                                                                 overwrite=False)\n",
    "        self.maps_downloader = map_downloader or Downloader(\"./data/salicon\", \"\", zip_filename=\"maps.zip\",\n",
    "                                                            overwrite=False)\n",
    "\n",
    "        self.images_downloader()\n",
    "        self.maps_downloader()\n",
    "\n",
    "        # 画像とマップのペアを取得\n",
    "        self.image_map_pair_cache = []\n",
    "        self.cache_image_map_paths()\n",
    "\n",
    "    def cache_image_map_paths(self):\n",
    "        for category in self.categories:\n",
    "            images_dir = self.images_downloader.extract_path\n",
    "            maps_dir = self.maps_downloader.extract_path\n",
    "\n",
    "            images_path_list = sorted(glob.glob(path.join(images_dir, category, \"*.jpg\")))\n",
    "            maps_path_list = sorted(glob.glob(path.join(maps_dir, category, \"*.png\")))\n",
    "\n",
    "            # ペアリング\n",
    "            for img_path, map_path in zip(images_path_list, maps_path_list):\n",
    "                if path.basename(img_path) == path.basename(map_path).replace(\".png\", \".jpg\"):\n",
    "                    self.image_map_pair_cache.append((img_path, map_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_map_pair_cache)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_path, map_path = self.image_map_pair_cache[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        map_image = Image.open(map_path).convert(\"RGB\")\n",
    "\n",
    "        if self.image_transform is not None:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "            if self.map_transform is not None:\n",
    "                map_image = self.map_transform(map_image)\n",
    "            else:\n",
    "                map_image = self.image_transform(map_image)\n",
    "\n",
    "        return image, map_image\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(\n",
    "            f\"image: {Image.open(pair[0]).size}, map: {Image.open(pair[1]).size}\" for pair in self.image_map_pair_cache)\n"
   ],
   "id": "d6ae05a98ab1f2cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## モデルの実装",
   "id": "ad64cc57ad529323"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T11:29:07.511702Z",
     "start_time": "2024-11-04T11:29:05.312083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn import Module, Conv2d, LeakyReLU, Upsample\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "class DecoderBlock(Module):\n",
    "    def __init__(self, in_channels=512, out_channels=512):\n",
    "        super().__init__()\n",
    "        self.module = Sequential(\n",
    "            Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            LeakyReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "class Generator(Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.encoder1 = vgg16(pretrained=pretrained).features[:17]\n",
    "        self.encoder_last = vgg16(pretrained=pretrained).features[17:-1]\n",
    "        self.decoder = Sequential(\n",
    "            DecoderBlock(512, 512),\n",
    "            DecoderBlock(512, 512),\n",
    "            DecoderBlock(512, 512),\n",
    "            Upsample(scale_factor=2),\n",
    "\n",
    "            DecoderBlock(512, 512),\n",
    "            DecoderBlock(512, 512),\n",
    "            DecoderBlock(512, 512),\n",
    "\n",
    "            Upsample(scale_factor=2),\n",
    "\n",
    "            DecoderBlock(512, 256),\n",
    "            DecoderBlock(256, 256),\n",
    "            DecoderBlock(256, 256),\n",
    "\n",
    "            Upsample(scale_factor=2),\n",
    "\n",
    "            DecoderBlock(256, 128),\n",
    "            DecoderBlock(128, 128),\n",
    "\n",
    "            Upsample(scale_factor=2),\n",
    "            DecoderBlock(128, 64),\n",
    "            DecoderBlock(64, 64),\n",
    "        )\n",
    "        self.output = Sequential(\n",
    "            Conv2d(64, 1, 1, padding=0),\n",
    "            Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder1(x)\n",
    "        x = self.encoder_last(x)\n",
    "        x = self.decoder(x)\n",
    "        return self.output(x)"
   ],
   "id": "54a588cf448ae4cb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.nn import Module, Sequential, Conv2d, LeakyReLU, Upsample, Sigmoid, MaxPool2d, Tanh, Linear\n",
    "\n",
    "\n",
    "class Discriminator(Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.main = Sequential(\n",
    "            Conv2d(4, 3, 1, padding=1),\n",
    "            LeakyReLU(inplace=True),\n",
    "            Conv2d(3, 32, 3, padding=1),\n",
    "            LeakyReLU(inplace=True),\n",
    "            MaxPool2d(2, stride=2),\n",
    "            Conv2d(32, 64, 3, padding=1),\n",
    "            LeakyReLU(inplace=True),\n",
    "            Conv2d(64, 64, 3, padding=1),\n",
    "            LeakyReLU(inplace=True),\n",
    "            MaxPool2d(2, stride=2),\n",
    "            Conv2d(64, 64, 3, padding=1),\n",
    "            LeakyReLU(inplace=True),\n",
    "            Conv2d(64, 64, 3, padding=1),\n",
    "            LeakyReLU(inplace=True),\n",
    "            MaxPool2d(2, stride=2))\n",
    "\n",
    "        self.classifier = Sequential(\n",
    "            Linear(64 * 32 * 24, 100, bias=True),\n",
    "            Tanh(),\n",
    "            Linear(100, 2, bias=True),\n",
    "            Tanh(),\n",
    "            Linear(2, 1, bias=True),\n",
    "            Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ],
   "id": "2a43797a7d994f3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### トレーニングの実装",
   "id": "c2f5853354be85bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def train(generator: Module, discriminator: Module, dataloader: DataLoader, criterion: Module, optimizer, device,\n",
    "          show_stride=100):\n",
    "    # set train mode\n",
    "    generator.train()\n",
    "\n",
    "    criterion.to(device)\n",
    "    optimizer.to(device)\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    for batch_index, (data, label) in enumerate(dataloader):\n",
    "        data, label = data.to(device), label.to(data)\n",
    "\n",
    "        loss = criterion(label)\n",
    "\n",
    "        if batch_index % show_stride == 0:\n",
    "            print(f\"[<train> batch: {batch_index}]\")\n"
   ],
   "id": "2df4803a3fa8995e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.nn import Module\n",
    "import torch\n",
    "\n",
    "\n",
    "def test(generator, discriminator, dataloader: DataLoader, criterion, device, show_stride=100):\n",
    "    criterion.to(device)\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    with no_grad():\n",
    "        for batch_index, (data, label) in enumerate(dataloader):\n",
    "            data, label = data.to(device), label.to(data)\n",
    "\n",
    "            loss = criterion(label)\n",
    "\n",
    "            if batch_index % show_stride == 0:\n",
    "                print(f\"[<train> batch: {batch_index}]\")"
   ],
   "id": "45ef0bc4740adefc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T12:15:18.837193Z",
     "start_time": "2024-11-04T12:15:18.834304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run(generator, discriminator, train_dataloader, criterion, optimizer, test_dataloader, device, epochs=120):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"--- train start ---\")\n",
    "        train(generator, discriminator, train_dataloader, criterion, optimizer, device)\n",
    "\n",
    "        print(\"--- test start ---\")\n",
    "        test(generator, discriminator, test_dataloader, criterion, device)\n"
   ],
   "id": "bfb5a0a1ff4e4c55",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### Utils",
   "id": "57f45855fa03a3f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from os.path import exists\n",
    "from os import makedirs\n",
    "\n",
    "\n",
    "def save_log(save_dir=\"./log\"):\n",
    "    if not exists(save_dir):\n",
    "        makedirs(save_dir)"
   ],
   "id": "e3f113f1b3997af8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_timestamp(date_format=\"{0:%Y%m%d-%H%M%S}\"):\n",
    "    return date_format.format(datetime.now())"
   ],
   "id": "f70a610be0cfb410"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict(generator: Module, transform, image):\n",
    "    with torch.no_grad():\n",
    "        image = transform(image)\n",
    "        saliency_map = generator(image)\n",
    "        saliency_map = np.array(saliency_map.cpu())[0, 0]\n",
    "\n",
    "    saliency_map = saliency_map / saliency_map.sum()\n",
    "    saliency_map = (())"
   ],
   "id": "6c4be839b6b991a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### セットアップ",
   "id": "828c1a7e3e44e482"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision.transforms import Compose\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adagrad\n",
    "\n",
    "transforms = Compose([ToTensor()])\n",
    "\n",
    "salicon_train = SALICONDataset(image_transform=transforms)\n",
    "salicon_test = SALICONDataset(image_transform=transforms, val_mode=True)\n",
    "\n",
    "train_dataloader = DataLoader(salicon_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(salicon_test, batch_size=8, shuffle=False)\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "criterion = BCELoss()\n",
    "generator_optimizer = Adagrad([{'params': generator.encoder_last.paramators()},\n",
    "                               {'params': generator.decoder.paramators()}])\n",
    "discriminator_optimizer = Adagrad(discriminator.parameters())"
   ],
   "id": "b2aae1c3dd4569db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 実行",
   "id": "64efe0ef57f29ff4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fe02567d7430cb2b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
